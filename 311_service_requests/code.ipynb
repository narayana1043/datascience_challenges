{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark session and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import col, udf\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import FloatType, BooleanType, IntegerType\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "spark_conf = SparkConf() \\\n",
    "    .setAll([\n",
    "         ['spark.serializer','org.apache.spark.serializer.KryoSerializer'],\n",
    "         ['spark.rdd.compress','true'],   \n",
    "    ])\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pager\") \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataframe and drop low level features\n",
    "save the processed file as parquet since it is a columnar format we can perfrom groupby operations faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 311_service_requests data from hdfs\n",
    "df = spark.read.csv('hdfs://ip-172-31-51-46.ec2.internal/pager/311_service_requests.csv', header=True,\n",
    "                   inferSchema=True, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None)\n",
    "\n",
    "# rename columns and convert to lower case\n",
    "for col, dtype in df.dtypes:\n",
    "    new_col = col.replace(\" \", \"\")\n",
    "    df = df.withColumnRenamed(col, new_col)\n",
    "    if dtype == 'string':\n",
    "        df = df.withColumn(new_col, functions.lower(df[new_col]))\n",
    "\n",
    "\n",
    "\n",
    "df = df.dropna(how='any', subset=['CreatedDate','ClosedDate'])\n",
    "\n",
    "# UDF fuctions\n",
    "get_month_func = udf(lambda x: datetime.strptime(x, '%m/%d/%Y %I:%M:%S %p').month, IntegerType())\n",
    "get_year_func = udf(lambda x: datetime.strptime(x, '%m/%d/%Y %I:%M:%S %p').year, IntegerType())\n",
    "time_difference_func = udf(lambda x, y: (datetime.strptime(x, '%m/%d/%Y %I:%M:%S %p') - \n",
    "                                         datetime.strptime(y, '%m/%d/%Y %I:%M:%S %p')).total_seconds()/3600, \n",
    "                           FloatType())\n",
    "is_school_func = udf(lambda x: True if x == \"Unspecified\" else False, BooleanType())\n",
    "\n",
    "\n",
    "# create new columns and select required columns\n",
    "df = df.withColumn('Month', get_month_func(df['CreatedDate']))\n",
    "df = df.withColumn('Year', get_year_func(df['CreatedDate']))\n",
    "df = df.withColumn('TimeTaken', time_difference_func(df['ClosedDate'], df['CreatedDate']))\n",
    "df = df.withColumn('SchoolZone', is_school_func(df['SchoolName']))\n",
    "\n",
    "# filters\n",
    "df.filter(df['TimeTaken']>0)\n",
    "df = df[['UniqueKey', 'Month', 'Year', 'TimeTaken', 'Agency', 'ComplaintType', 'Descriptor', 'LocationType',\n",
    "         'Incidentzip', 'AddressType', 'City', 'FacilityType', 'Borough', 'Status', 'SchoolZone', 'CreatedDate',\n",
    "         'ClosedDate']]\n",
    "\n",
    "df.write.parquet('hdfs://ip-172-31-51-46.ec2.internal/pager/parquet/311_data', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    " Group based on the each of the selected column for selected column + Time Taken column pair\n",
    " \n",
    " For each grouping use aggregate for count and mean\n",
    " \n",
    " Convert the grouped data into pandas dataframes and write into excel sheets\n",
    " \n",
    " Perfrom exploratory analysis means of grouped data by joining all the pandas dataframes formed\n",
    " \n",
    " Check for variance in the means\n",
    " \n",
    "  (i) Large variance/stddev implies that the feature is important driver because it means that in each \n",
    "      feature the category are diverse and have extreme varying effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"hdfs://ip-172-31-51-46.ec2.internal/pager/parquet/311_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_counts_dict = {}\n",
    "pandas_df_dict = {}\n",
    "\n",
    "select_columns = ['Agency', 'ComplaintType', 'LocationType', 'AddressType', 'City', 'FacilityType', 'Borough', \n",
    "                  'Incidentzip', 'Status', 'SchoolZone']\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('./pager_analysis.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for col in select_columns:\n",
    "    group = [col] + ['TimeTaken']\n",
    "    grouped = df.select(group).groupBy(col)\n",
    "    grouped_counts_dict[col] = {}\n",
    "    grouped_counts_dict[col]['counts'] = grouped.count()\n",
    "    grouped_counts_dict[col]['mean_time_taken'] = grouped.mean('TimeTaken')\n",
    "\n",
    "    pandas_df_count = grouped_counts_dict[col]['counts'].toPandas()\n",
    "    pandas_df_mean = grouped_counts_dict[col]['mean_time_taken'].toPandas()\n",
    "    \n",
    "    pandas_df = pandas_df_count.join(pandas_df_mean.set_index(col), on=col)\n",
    "    pandas_df_dict[col] = pandas_df[['avg(TimeTaken)']].describe()\n",
    "    \n",
    "    pandas_df.to_excel(writer, sheet_name=col)\n",
    "\n",
    "exploratory_analysis = pd.concat([pandas_df_dict[col] for col in select_columns], axis=1).reset_index()\n",
    "exploratory_analysis.columns = ['TimeTaken'] + select_columns\n",
    "exploratory_analysis.to_excel(writer, sheet_name='exploratory_analysis')\n",
    "\n",
    "display(exploratory_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
